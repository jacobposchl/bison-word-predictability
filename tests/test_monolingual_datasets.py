"""
Test script to validate monolingual datasets generated by preprocessing pipeline.

This script analyzes the 4 monolingual CSV files to ensure:
1. Data structure is correct
2. Patterns are monolingual (only C or only E)
3. Filler handling is correct (WITH vs WITHOUT)
4. No data corruption or unexpected values
5. Consistency across datasets
"""

import sys
from pathlib import Path
import pandas as pd
import logging

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.core.config import Config

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def validate_pattern(df: pd.DataFrame, expected_lang: str, dataset_name: str) -> bool:
    """
    Validate that all patterns in the dataset match expected language.
    
    Args:
        df: DataFrame to validate
        expected_lang: 'C' for Cantonese or 'E' for English
        dataset_name: Name of dataset for logging
        
    Returns:
        True if all patterns are valid, False otherwise
    """
    logger.info(f"\n{'='*80}")
    logger.info(f"VALIDATING PATTERNS: {dataset_name}")
    logger.info(f"{'='*80}")
    
    other_lang = 'E' if expected_lang == 'C' else 'C'
    errors = []
    
    for idx, row in df.iterrows():
        pattern = row.get('pattern', '')
        
        # Check if pattern contains expected language
        if expected_lang not in pattern:
            errors.append(f"Row {idx}: Pattern '{pattern}' missing {expected_lang}")
        
        # Check if pattern contains other language (should not!)
        if other_lang in pattern:
            errors.append(f"Row {idx}: Pattern '{pattern}' contains {other_lang} (code-switched!)")
        
        # Check for filler-only patterns
        if pattern == 'FILLER_ONLY':
            errors.append(f"Row {idx}: Pattern is 'FILLER_ONLY' (should be filtered out)")
        
        # Check for empty patterns
        if not pattern or pattern.strip() == '':
            errors.append(f"Row {idx}: Pattern is empty")
    
    if errors:
        logger.error(f"Found {len(errors)} pattern errors:")
        for error in errors[:10]:  # Show first 10 errors
            logger.error(f"  {error}")
        if len(errors) > 10:
            logger.error(f"  ... and {len(errors) - 10} more errors")
        return False
    else:
        logger.info(f"✓ All {len(df)} patterns are valid {expected_lang}-only patterns")
        return True


def validate_filler_handling(df: pd.DataFrame, should_have_fillers: bool, dataset_name: str) -> bool:
    """
    Validate filler handling in the dataset.
    
    Args:
        df: DataFrame to validate
        should_have_fillers: True if WITH fillers dataset, False if WITHOUT
        dataset_name: Name of dataset for logging
        
    Returns:
        True if filler handling is correct, False otherwise
    """
    logger.info(f"\n{'='*80}")
    logger.info(f"VALIDATING FILLER HANDLING: {dataset_name}")
    logger.info(f"{'='*80}")
    
    errors = []
    filler_stats = {
        'has_fillers_true': 0,
        'has_fillers_false': 0,
        'filler_count_zero': 0,
        'filler_count_nonzero': 0
    }
    
    for idx, row in df.iterrows():
        has_fillers = row.get('has_fillers', False)
        filler_count = row.get('filler_count', 0)
        pattern = row.get('pattern', '')
        
        # Update stats
        if has_fillers:
            filler_stats['has_fillers_true'] += 1
        else:
            filler_stats['has_fillers_false'] += 1
        
        if filler_count == 0:
            filler_stats['filler_count_zero'] += 1
        else:
            filler_stats['filler_count_nonzero'] += 1
        
        # Validate consistency
        if has_fillers and filler_count == 0:
            errors.append(f"Row {idx}: has_fillers=True but filler_count=0")
        
        if not has_fillers and filler_count > 0:
            errors.append(f"Row {idx}: has_fillers=False but filler_count={filler_count}")
        
        # For WITHOUT fillers dataset, check if fillers appear in pattern
        if not should_have_fillers and 'FILLER' in pattern:
            errors.append(f"Row {idx}: WITHOUT dataset has 'FILLER' in pattern '{pattern}'")
    
    logger.info(f"Filler statistics:")
    logger.info(f"  has_fillers=True:  {filler_stats['has_fillers_true']} ({filler_stats['has_fillers_true']/len(df)*100:.1f}%)")
    logger.info(f"  has_fillers=False: {filler_stats['has_fillers_false']} ({filler_stats['has_fillers_false']/len(df)*100:.1f}%)")
    logger.info(f"  filler_count=0:    {filler_stats['filler_count_zero']} ({filler_stats['filler_count_zero']/len(df)*100:.1f}%)")
    logger.info(f"  filler_count>0:    {filler_stats['filler_count_nonzero']} ({filler_stats['filler_count_nonzero']/len(df)*100:.1f}%)")
    
    if errors:
        logger.error(f"Found {len(errors)} filler handling errors:")
        for error in errors[:10]:
            logger.error(f"  {error}")
        if len(errors) > 10:
            logger.error(f"  ... and {len(errors) - 10} more errors")
        return False
    else:
        logger.info(f"✓ Filler handling is correct")
        return True


def validate_data_structure(df: pd.DataFrame, dataset_name: str) -> bool:
    """
    Validate that the DataFrame has the expected columns and data types.
    
    Args:
        df: DataFrame to validate
        dataset_name: Name of dataset for logging
        
    Returns:
        True if structure is valid, False otherwise
    """
    logger.info(f"\n{'='*80}")
    logger.info(f"VALIDATING DATA STRUCTURE: {dataset_name}")
    logger.info(f"{'='*80}")
    
    expected_columns = [
        'reconstructed_sentence',
        'sentence_original',
        'pattern',
        'matrix_language',
        'group_code',
        'group',
        'participant_id',
        'filler_count',
        'has_fillers'
    ]
    
    missing_columns = [col for col in expected_columns if col not in df.columns]
    extra_columns = [col for col in df.columns if col not in expected_columns]
    
    if missing_columns:
        logger.error(f"Missing columns: {missing_columns}")
        return False
    
    if extra_columns:
        logger.warning(f"Extra columns (not expected): {extra_columns}")
    
    logger.info(f"✓ All expected columns present")
    logger.info(f"  Columns: {list(df.columns)}")
    
    # Check for null values
    null_counts = df.isnull().sum()
    if null_counts.sum() > 0:
        logger.warning(f"Found null values:")
        for col, count in null_counts[null_counts > 0].items():
            logger.warning(f"  {col}: {count} nulls ({count/len(df)*100:.1f}%)")
    else:
        logger.info(f"✓ No null values found")
    
    # Check for duplicate sentences
    duplicates = df[df.duplicated(subset=['reconstructed_sentence'], keep=False)]
    if len(duplicates) > 0:
        logger.error(f"Found {len(duplicates)} duplicate sentences!")
        logger.error(f"Example duplicates:")
        for sent in duplicates['reconstructed_sentence'].unique()[:3]:
            dup_rows = df[df['reconstructed_sentence'] == sent]
            logger.error(f"  '{sent}' appears {len(dup_rows)} times")
        return False
    else:
        logger.info(f"✓ No duplicate sentences found")
    
    return True


def analyze_dataset(csv_path: str, expected_lang: str, should_have_fillers: bool, dataset_name: str) -> bool:
    """
    Comprehensive analysis of a monolingual dataset.
    
    Args:
        csv_path: Path to CSV file
        expected_lang: 'C' for Cantonese or 'E' for English
        should_have_fillers: True if WITH fillers, False if WITHOUT
        dataset_name: Name for logging
        
    Returns:
        True if all validations pass, False otherwise
    """
    logger.info(f"\n\n{'#'*80}")
    logger.info(f"# ANALYZING: {dataset_name}")
    logger.info(f"# File: {csv_path}")
    logger.info(f"{'#'*80}\n")
    
    # Check if file exists
    if not Path(csv_path).exists():
        logger.error(f"File not found: {csv_path}")
        logger.error("Run preprocessing first: python scripts/preprocess/preprocess.py")
        return False
    
    # Load CSV
    try:
        df = pd.read_csv(csv_path)
        logger.info(f"Loaded {len(df)} sentences from {Path(csv_path).name}")
    except Exception as e:
        logger.error(f"Error loading CSV: {e}")
        return False
    
    # Run validations
    results = []
    results.append(validate_data_structure(df, dataset_name))
    results.append(validate_pattern(df, expected_lang, dataset_name))
    results.append(validate_filler_handling(df, should_have_fillers, dataset_name))
    
    # Sample data
    logger.info(f"\n{'='*80}")
    logger.info(f"SAMPLE DATA (first 3 rows): {dataset_name}")
    logger.info(f"{'='*80}")
    for idx, row in df.head(3).iterrows():
        logger.info(f"\nRow {idx}:")
        logger.info(f"  Sentence: {row['reconstructed_sentence']}")
        logger.info(f"  Pattern:  {row['pattern']}")
        logger.info(f"  Matrix:   {row['matrix_language']}")
        logger.info(f"  Fillers:  has_fillers={row['has_fillers']}, count={row['filler_count']}")
        logger.info(f"  Participant: {row['participant_id']} ({row['group']})")
    
    # Final result
    all_passed = all(results)
    logger.info(f"\n{'='*80}")
    if all_passed:
        logger.info(f"✓✓✓ {dataset_name}: ALL VALIDATIONS PASSED ✓✓✓")
    else:
        logger.error(f"✗✗✗ {dataset_name}: SOME VALIDATIONS FAILED ✗✗✗")
    logger.info(f"{'='*80}")
    
    return all_passed


def main():
    """Main test function."""
    logger.info("\n" + "="*80)
    logger.info("MONOLINGUAL DATASET VALIDATION TEST")
    logger.info("="*80)
    
    # Load config
    config = Config()
    
    # Define datasets to test
    datasets = [
        {
            'name': 'Cantonese Monolingual WITH Fillers',
            'path': config.get_csv_cantonese_mono_with_fillers_path(),
            'lang': 'C',
            'fillers': True
        },
        {
            'name': 'Cantonese Monolingual WITHOUT Fillers',
            'path': config.get_csv_cantonese_mono_without_fillers_path(),
            'lang': 'C',
            'fillers': False
        },
        {
            'name': 'English Monolingual WITH Fillers',
            'path': config.get_csv_english_mono_with_fillers_path(),
            'lang': 'E',
            'fillers': True
        },
        {
            'name': 'English Monolingual WITHOUT Fillers',
            'path': config.get_csv_english_mono_without_fillers_path(),
            'lang': 'E',
            'fillers': False
        }
    ]
    
    # Run tests on each dataset
    results = {}
    for dataset in datasets:
        passed = analyze_dataset(
            dataset['path'],
            dataset['lang'],
            dataset['fillers'],
            dataset['name']
        )
        results[dataset['name']] = passed
    
    # Final summary
    logger.info("\n\n" + "="*80)
    logger.info("FINAL SUMMARY")
    logger.info("="*80)
    
    for dataset_name, passed in results.items():
        status = "✓ PASS" if passed else "✗ FAIL"
        logger.info(f"{status}: {dataset_name}")
    
    all_passed = all(results.values())
    logger.info("\n" + "="*80)
    if all_passed:
        logger.info("✓✓✓ ALL DATASETS VALIDATED SUCCESSFULLY! ✓✓✓")
        logger.info("="*80)
        return 0
    else:
        logger.error("✗✗✗ SOME DATASETS FAILED VALIDATION ✗✗✗")
        logger.error("="*80)
        return 1


if __name__ == '__main__':
    exit_code = main()
    sys.exit(exit_code)
